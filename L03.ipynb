{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание к лекции 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?\n",
    "\n",
    "Ответ: \n",
    "    1. micro вычисляет метрику глобально по всем объектам, не учитывает разбалансировку классов\n",
    "\n",
    "    2. macro вычисляет метрику для каждого объекта и потом находит их невзвешенное срежнее, не учитывает разбалансировку классов\n",
    "\n",
    "    3. weighted делает то же самое что и macro, но дополнительно учитывает дизбаланс классов\n",
    "\n",
    "\n",
    "2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?\n",
    "\n",
    "Ответ:\n",
    "   - LightGBM использует технику односторонней выборки на основе градиента (GOSS) для фильтрации экземпляров данных для нахождения значения разделения\n",
    "   - XGBoost использует предварительную сортировку и с помощью алгоритма на базе гистограммы вычисляет наилучшие разделения\n",
    "   - CatBoost обладает гибкостью, позволяя задавать индексы категориальных столбцов, есть динамический интерфейс\n",
    "   - LightGBM также может обрабатывать категориальные признаки, но перед построением набора данных для LGBM необходимо преобразовать категориальные признаки в тип int\n",
    "   - XGBoost не может обрабатывать категориальные функции сам по себе, он принимает только числовые значения, подобные случайному лесу. Поэтому перед подачей категориальных данных в XGBoost необходимо выполнить различные кодировки, такие как кодирование меток, среднее кодирование или однократное кодирование.\n",
    "   - Сравнение гиперпараметров:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Функции\n",
    "        </td>\n",
    "        <td>\n",
    "        XGBoost\n",
    "        </td>\n",
    "        <td>\n",
    "        CatBoost\n",
    "        </td>\n",
    "        <td>\n",
    "        LightGBM\n",
    "        </td>\n",
    "    </tr>\n",
    " <tr>\n",
    "        <td>\n",
    "        Важные гиперпараметры\n",
    "        </td>\n",
    "        <td>\n",
    "            1. Learning_rate or eta - оптимальное значение  0.01-02\n",
    "            2. max_depth\n",
    "            3. min_child_weight: аналогично min_child_leaf; по умолчанию 1\n",
    "        </td>\n",
    "        <td>\n",
    "            1. Learning_rate\n",
    "            2. Dept - до 16, оптимальное 1-10\n",
    "            3. нет параметра min_child_weight\n",
    "            4. l2-leaf-reg: L2  коэффициент регуляризации\n",
    "        </td>\n",
    "        <td>\n",
    "            1. Learning_rate\n",
    "            2. max_depth: по умлочанию 20 и надо обращать внимание на num_leaves, который длжен быть меньше чем 2^max_depth\n",
    "            3. min_data_in_leaf: по умолчанию 20, alias = min_data, min_child_samples\n",
    "        </td>\n",
    "    </tr>    \n",
    "<tr>\n",
    "        <td>\n",
    "        Параметры для работы с категориальными признаками\n",
    "        </td>\n",
    "        <td>\n",
    "            не поддерживает\n",
    "        </td>\n",
    "        <td>\n",
    "            1. cat_features: указываем индексы категориальных переменных\n",
    "            2. one_hot_max_size: используем унитарный код (one hot encoding) для признаков, максимальная длина 255\n",
    "        </td>\n",
    "        <td>\n",
    "            1. categorial_feture: прямо указывая категориальный признак\n",
    "        </td>\n",
    " </tr> \n",
    "<tr>\n",
    "        <td>\n",
    "        Параметры для контролирования скорости\n",
    "        </td>\n",
    "        <td>\n",
    "            1. colsample_bytree: соотношение подвыборки столбцов\n",
    "            2. subsample: коэффициент подвыборки обучающего экземпляра\n",
    "            3. n_estimation: макимальное количество решающих деревьев, если задать много то будет переобучение\n",
    "        </td>\n",
    "        <td>\n",
    "            1. rsm: метод случайного подпространства. Процент признаков для использования в каждом разделении выброки\n",
    "            2. Нет такого параметра как подмножество\n",
    "            3. iteration: максимальное количество деревьев, если задато много будет переобучение\n",
    "        </td>\n",
    "        <td>\n",
    "            1. feature_fraction: доля признаков которая будет использована при каждой итерации\n",
    "            2. bagging_fraction: часть данных используемая при каждой итерации, регулируем скорость и обезопасим от переобучения\n",
    "            3. num_iteration: количество итераций бустинга, по умолчанию 100\n",
    "        </td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "часть курсовой работы :\n",
    "\n",
    "- Отбор признаков\n",
    "- Балансировка классов\n",
    "- Подбор моделей, получение бейзлана\n",
    "\n",
    "https://github.com/sols-git/Python_Data-Science/blob/master/course_project.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
